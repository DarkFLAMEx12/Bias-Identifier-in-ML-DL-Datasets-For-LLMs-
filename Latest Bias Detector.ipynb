{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a39711ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NANDAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NANDAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified dataset saved as 'modified_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "data = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "def detect_negative_sentiment(text):\n",
    "    result = sentiment_model(text)[0]\n",
    "    is_negative = result['label'] == 'NEGATIVE'\n",
    "    sentiment_score = result['score']\n",
    "    return is_negative, sentiment_score\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "\n",
    "def preprocess_text(sentences):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.tokenize(sentence.lower())\n",
    "        tokens = [word for word in tokens if word not in stop_words and len(word) > 1 and not word.isdigit()]\n",
    "        processed_sentences.append(\" \".join(tokens))\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "def vectorize_text(processed_sentences):\n",
    "    cv = CountVectorizer(ngram_range=(1, 2), binary=True)\n",
    "    X = cv.fit_transform(processed_sentences)\n",
    "    return X, cv\n",
    "\n",
    "def train_model(X, y):\n",
    "    model = SVC(kernel='linear')\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def neutralize_with_gpt(sentence):\n",
    "    prompt = f\"Please neutralize the following biased sentence: {sentence}\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = gpt_model.generate(inputs, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7, do_sample=True)\n",
    "    neutralized_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    neutralized_sentence = neutralized_sentence.replace(prompt, \"\").strip()\n",
    "    return neutralized_sentence\n",
    "\n",
    "def analyze_and_modify_bias_in_text(document, model, vectorizer):\n",
    "    sentences = sent_tokenize(document)\n",
    "    processed_sentences = preprocess_text(sentences)\n",
    "    X_new = vectorizer.transform(processed_sentences)\n",
    "    predictions = model.predict(X_new)\n",
    "\n",
    "    modified_sentences = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentiment, sentiment_score = detect_negative_sentiment(sentence)\n",
    "        if sentiment and sentiment_score >= 0.75:\n",
    "            modified_sentence = neutralize_with_gpt(sentence)\n",
    "            modified_sentences.append(modified_sentence)\n",
    "        else:\n",
    "            modified_sentences.append(sentence)\n",
    "\n",
    "    return modified_sentences\n",
    "\n",
    "processed_sentences = preprocess_text(data[\"sentence\"])\n",
    "X, cv = vectorize_text(processed_sentences)\n",
    "y = data[\"label\"]\n",
    "\n",
    "model = train_model(X, y)\n",
    "\n",
    "with open(\"vocabulary.json\", \"w\") as f:\n",
    "    json.dump(cv.vocabulary_, f)\n",
    "\n",
    "document = '''People from certain regions are often stereotyped as being lazy or unmotivated. For example, many people believe that individuals from rural areas are less industrious than those living in cities. This stereotype often leads to discrimination in the job market, as urban workers are seen as more capable and hardworking.\n",
    "Additionally, there are assumptions about the intelligence of people based on where they were born. Some believe that individuals from less developed countries are less educated, which contributes to a lack of opportunity and inequality. This kind of bias can also affect the way people are treated in social and professional settings, even when they possess the same qualifications and skills as others.\n",
    "\n",
    "'''\n",
    "\n",
    "modified_sentences = analyze_and_modify_bias_in_text(document, model, vectorizer=cv)\n",
    "\n",
    "modified_df = pd.DataFrame({\n",
    "    'modified_sentence': modified_sentences\n",
    "})\n",
    "\n",
    "modified_df.to_csv('modified_dataset.csv', index=False)\n",
    "\n",
    "print(\"Modified dataset saved as 'modified_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ade276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
